<h1 align="center"> üåü Vision-Transformer-Papers üåü </h1>

<p align="center">
  </a> 
    </a>
  <em>
    Image Classification
    ¬∑ Semantic Segmentation
  </em>
  <br />
  <em>
    Object Detection
    ¬∑ General Classification
  </em>
  <br />
  <em>
     Fine-Grained Image Classification
     ¬∑  Depth Estimation
  </em>
  <br />
  <em>
    Instance Segmentation	
    ¬∑ Salient Object Detection
    ¬∑ Person Re-Identification	
  </em>
  <br />
  
  <em>
    <a href="https://github.com/gyunggyung/PyTorch">
      GOTO PyTorch!
    </a>
  </em>
</p>

<p align="center">
  <a href="https://opensource.org/licenses/MIT">
    <img alt="licenses" src="https://img.shields.io/github/license/gyunggyung/Vision-Transformer-Papers?style=flat-square"></a>
  <a href="https://github.com/gyunggyung/Vision-Transformer-Papers/stargazers">
    <img alt="GitHub stars" src="https://img.shields.io/github/stars/gyunggyung/Vision-Transformer-Papers?style=flat-square&color=yellow"></a>
  <a href="https://github.com/gyunggyung/Vision-Transformer-Papers/blob/master/watchers">
    <img alt="GitHub watching" src="https://img.shields.io/github/watchers/gyunggyung/Vision-Transformer-Papers?style=flat-square&color=ff69b4"></a>
  <a href="https://github.com/gyunggyung/Vision-Transformer-Papers/graphs/contributors">
    <img alt="contributors" src="https://img.shields.io/badge/contributors-welcome-yellowgreen?style=flat-square"></a>
</p>

<div align="center">
    <sub> Let's find out the latest and various Vision-Transformer-related papers. üôá‚Äç‚ôÇÔ∏èüôá‚Äç‚ôÄÔ∏è by <a href="https://github.com/gyunggyung/Vision-Transformer-Papers/stargazers">Stargazers</a>  </sub>
</div>


## Papers

- [2020/10] **[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929v2.pdf)**: *ViT*
- [2019/08] **[VISUALBERT: A SIMPLE AND PERFORMANT BASELINE FOR VISION AND LANGUAGE](https://arxiv.org/abs/2010.11929v2.pdf)**: *VISUALBERT*
- [2021/08] **[Boosting Salient Object Detection with Transformer-based Asymmetric Bilateral U-Net](https://arxiv.org/pdf/2108.07851v2.pdf)**: *U-Net*
- [2021/03] **[Dynamic Feature Regularized Loss for Weakly Supervised Semantic Segmentation](https://arxiv.org/pdf/2108.01296v1.pdf)**
- [2020/11] **[AdaBins: Depth Estimation using Adaptive Bins](https://arxiv.org/pdf/2011.14141v1.pdf)**: *AdaBins*
- [2021/04] **[VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text](https://arxiv.org/pdf/2104.11178v2.pdf)**: *VATT*
- [2021/05] **[CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification](https://arxiv.org/pdf/2103.14899v1.pdf)**: *CrossViT*
- [2021/04] **[Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/pdf/2104.14294v2.pdf)**
- [2021/06] **[PVTv2: Improved Baselines with Pyramid Vision Transformer](https://arxiv.org/pdf/2106.13797v4.pdf)**: *PVTv2*
- [2021/01] **[Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet](https://arxiv.org/pdf/2101.11986v2.pdf)**: *Tokens-to-Token ViT*
- [2021/04] **[Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/pdf/2104.14294v2.pdf)**
- [2021/06] **[BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/pdf/2106.08254v1.pdf)**: *BEiT*
- [2021/03] **[Vision Transformers for Dense Prediction](https://arxiv.org/pdf/2103.13413v1.pdf)**
- [2021/02] **[TransReID: Transformer-based Object Re-Identification](https://arxiv.org/pdf/2102.04378v2.pdf)**: *TransReID*
- [2021/04] **[All Tokens Matter: Token Labeling for Training Better Vision Transformers](https://arxiv.org/pdf/2104.10858v3.pdf)**
- [2021/07] **[AutoFormer: Searching Transformers for Visual Recognition](https://arxiv.org/pdf/2107.00651v1.pdf)**: *AutoFormer*
- [2021/04] **[Twins: Revisiting the Design of Spatial Attention in Vision Transformers](https://arxiv.org/pdf/2104.13840v3.pdf)**: *Twins*
- [2021/06] **[You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/pdf/2106.00666v2.pdf)**

- [] **[]()**: **


## Reference
- https://huggingface.co/transformers/model_doc/vit.html
- https://paperswithcode.com/method/vision-transformer
- https://github.com/gyunggyung/NLP-Papers
