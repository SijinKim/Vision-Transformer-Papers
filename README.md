<h1 align="center"> üåü Vision-Transformer-Papers üåü </h1>

<p align="center">
  </a> 
    </a>
  <em>
    Image Classification
    ¬∑ Semantic Segmentation
  </em>
  <br />
  <em>
    Object Detection
    ¬∑ General Classification
  </em>
  <br />
  <em>
     Fine-Grained Image Classification
     ¬∑  Depth Estimation
  </em>
  <br />
  <em>
    Instance Segmentation	
    ¬∑ Salient Object Detection
    ¬∑ Person Re-Identification	
  </em>
  <br />
  
  <em>
    <a href="https://github.com/gyunggyung/PyTorch">
      GOTO PyTorch!
    </a>
  </em>
</p>

<p align="center">
  <a href="https://opensource.org/licenses/MIT">
    <img alt="licenses" src="https://img.shields.io/github/license/gyunggyung/Vision-Transformer-Papers?style=flat-square"></a>
  <a href="https://github.com/gyunggyung/Vision-Transformer-Papers/stargazers">
    <img alt="GitHub stars" src="https://img.shields.io/github/stars/gyunggyung/Vision-Transformer-Papers?style=flat-square&color=yellow"></a>
  <a href="https://github.com/gyunggyung/Vision-Transformer-Papers/blob/master/watchers">
    <img alt="GitHub watching" src="https://img.shields.io/github/watchers/gyunggyung/Vision-Transformer-Papers?style=flat-square&color=ff69b4"></a>
  <a href="https://github.com/gyunggyung/Vision-Transformer-Papers/graphs/contributors">
    <img alt="contributors" src="https://img.shields.io/badge/contributors-welcome-yellowgreen?style=flat-square"></a>
</p>

<div align="center">
    <sub> Let's find out the latest and various Vision-Transformer-related papers. üôá‚Äç‚ôÇÔ∏èüôá‚Äç‚ôÄÔ∏è by <a href="https://github.com/gyunggyung/Vision-Transformer-Papers/stargazers">Stargazers</a>  </sub>
</div>


## Papers

- [2020/10] **[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929v2.pdf)**    
- [2019/08] **[VISUALBERT: A SIMPLE AND PERFORMANT BASELINE FOR VISION AND LANGUAGE](https://arxiv.org/abs/2010.11929v2.pdf)**: *VISUALBERT*
- [2021/08] : [Boosting Salient Object Detection with Transformer-based Asymmetric Bilateral U-Net](https://arxiv.org/pdf/2108.07851v2.pdf): *U-Net*
- [2021/03] : [Dynamic Feature Regularized Loss for Weakly Supervised Semantic Segmentation](https://arxiv.org/pdf/2108.01296v1.pdf)
- [2020/11] : [AdaBins: Depth Estimation using Adaptive Bins](https://arxiv.org/pdf/2011.14141v1.pdf): *AdaBins*
- [2021/04] : [VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text](https://arxiv.org/pdf/2104.11178v2.pdf): *VATT*
- [2021/05] : [CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification](https://arxiv.org/pdf/2103.14899v1.pdf): *CrossViT*
- [2021/04] : [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/pdf/2104.14294v2.pdf)
- [2021/06] : [PVTv2: Improved Baselines with Pyramid Vision Transformer](https://arxiv.org/pdf/2106.13797v4.pdf): *PVTv2*
- [2021/01] : [Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet](https://arxiv.org/pdf/2101.11986v2.pdf): *Tokens-to-Token ViT*
- [] : [](): **

## Reference
- https://huggingface.co/transformers/model_doc/vit.html
- https://paperswithcode.com/method/vision-transformer
- https://github.com/gyunggyung/NLP-Papers
